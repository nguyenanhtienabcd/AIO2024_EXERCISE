{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenanhtienabcd/AIO2024_EXERCISE/blob/feature%2FMODULE7-WEEK2/m07w02_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZH1FCZXhY3z"
      },
      "source": [
        "### import các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujgqj19zaT6u",
        "outputId": "4f4adca3-cc4e-40c2-b44c-e84fe550abc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXLayhqld6T0",
        "outputId": "80dce460-190b-4b0c-8d63-4c812c38df6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/andrewmvd/dog-and-cat-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.03G/1.03G [00:21<00:00, 51.6MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print('Path to dataset files:', data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6XztWSiaiVK",
        "outputId": "77ed5824-5cff-4269-abf4-83ca32fb057f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/andrewmvd/dog-and-cat-detection\n",
            "License(s): CC-BY-SA-4.0\n",
            "Downloading dog-and-cat-detection.zip to ./dataset\n",
            " 99% 1.02G/1.03G [00:09<00:00, 120MB/s]\n",
            "100% 1.03G/1.03G [00:10<00:00, 110MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d andrewmvd/dog-and-cat-detection -p ./dataset --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulHQPpojjPS_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h86U9bWZVue"
      },
      "source": [
        "*   Khởi tạo\n",
        "*   Lọc dữ liệu\n",
        "*   Đếm object\n",
        "*   lấy mẫu\n",
        "*   phân tích annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbobIa3Ejw3T"
      },
      "outputs": [],
      "source": [
        "from genericpath import isfile\n",
        "# định nghĩa một lớp class image mới\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "  #------ khởi tạo dữ liệu---------\n",
        "  def __init__(self, annotations_dir, image_dir, transform = None):\n",
        "    self.annotations_dir = annotations_dir\n",
        "    self.image_dir = image_dir\n",
        "    self.transform = transform\n",
        "    self.image_files = self.filter_image_with_multiple_objects()\n",
        "\n",
        "  #------ lọc dữ liệu---------\n",
        "  def filter_image_with_multiple_objects(self):\n",
        "    # tạo một list chứa các file thỏa mãn\n",
        "    valid_image_files = []\n",
        "    for f in os.listdir(self.image_dir):\n",
        "      if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "        img_name = f\n",
        "        # mục đích của đoạn code này là chia file thành 2 phần\n",
        "        # [file name, extention] => [0] là lấy file name\n",
        "        annotation_name = img_name.split('.')[0] + '.xml'\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        # tạo một hàm dữ lại các ảnh có object đơn\n",
        "        if self.count_objects_in_annotation(annotation_path) <=1:\n",
        "          valid_image_files.append(img_name)\n",
        "        else:\n",
        "          print(f'Image {img_name} has multiple objects and will be excluded the dataset')\n",
        "    return valid_image_files\n",
        "\n",
        "   #------ đếm đối tượng---------\n",
        "  def count_objects_in_annotation(self, annotation_path):\n",
        "    try:\n",
        "      tree = ET.parse(annotation_path) # đọc file xml từ path\n",
        "      root = tree.getroot() # lấy phần gốc của file xml\n",
        "      count = 0\n",
        "      for obj in root.findall('object'):\n",
        "        count += 1\n",
        "      return count\n",
        "    except Exception as e:\n",
        "      print(f'Error reading annotation file {annotation_path}: {e}')\n",
        "      return 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  #------ lấy mẫu---------\n",
        "  def __getitem__(self, idx):\n",
        "    # đường dẫn ảnh\n",
        "    img_name = self.image_files[idx]\n",
        "    img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "    # load ảnh lên\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # annotation path\n",
        "    annotation_name = img_name.split('.')[0] + '.xml'\n",
        "    annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "    # Parse annotation\n",
        "    label = self.parse_annotation(annotation_path)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, label\n",
        "\n",
        "    #------ phân tích annotations---------\n",
        "  def parse_annotation(self, annotation_path): # un-indent this method\n",
        "      tree = ET.parse(annotation_path)\n",
        "      root = tree.getroot()\n",
        "\n",
        "      label = None\n",
        "      for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        if (label is None):\n",
        "          # lấy label đầu tiên\n",
        "          label = name\n",
        "\n",
        "        label_num = 0 if label == 'cat' else 1 if label == 'dog' else -1\n",
        "        return label_num # This return statement may need to be outside the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8Ea2CGNoYxNX"
      },
      "outputs": [],
      "source": [
        "# data directory\n",
        "annotations_dir = os.path.join(data_dir, 'annotations')\n",
        "image_dir = os.path.join(data_dir, 'images')\n",
        "\n",
        "# lấy  dánh sách các files và tạo một dạng dữ liệu để chia dữ liệu\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name': image_files})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEEb4OMGkKHR"
      },
      "outputs": [],
      "source": [
        "# chi tập dữ liệu\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PH9x6MK4kYB9",
        "outputId": "5833583f-503b-4c09-f52c-ae340aca3e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Cats_Test736.png has multiple objects and will be excluded the dataset\n",
            "Image Cats_Test736.png has multiple objects and will be excluded the dataset\n",
            "Image Cats_Test736.png has multiple objects and will be excluded the dataset\n"
          ]
        }
      ],
      "source": [
        "# transform và chuyển dữ liệu sang tensor\n",
        "from torchvision import transforms\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "#train_dataset, val_dataset, test_dataset có cùng một tệp dữ liệu\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
        "test_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
        "\n",
        "# lọc dataset\n",
        "# lọc tất cả các file nào nằm trong dataframe train, val, test\n",
        "train_dataset.image_files =  [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files =  [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "test_dataset.image_files =  [f for f in test_dataset.image_files if f in test_df['image_name'].values]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN9YK-82zbFm"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDblTG5kzlM7"
      },
      "source": [
        "xây dựng model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_kX4G6f1zkDa",
        "outputId": "015691f2-dea1-4a82-c9d4-65508ca0c9de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 121MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimiser\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# show model sumary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jy7c2gm1XQ7"
      },
      "source": [
        "training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBq8rB211I2T",
        "outputId": "575888cf-db02-420d-91a0-6a9f97aabccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10],Validation Accuracy: 1.0000\n",
            "Epoch [2/10],Validation Accuracy: 1.0000\n",
            "Epoch [3/10],Validation Accuracy: 0.5000\n",
            "Epoch [4/10],Validation Accuracy: 1.0000\n",
            "Epoch [5/10],Validation Accuracy: 1.0000\n",
            "Epoch [6/10],Validation Accuracy: 1.0000\n",
            "Epoch [7/10],Validation Accuracy: 0.5000\n",
            "Epoch [8/10],Validation Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for data, targets in val_loader:\n",
        "      output = model(data)\n",
        "      loss = criterion(output, targets)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "      _,predictions = output.max(1)\n",
        "      correct = (predictions == targets).sum().item()\n",
        "      total = targets.size(0)\n",
        "      accuracy = correct / total\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}],Validation Accuracy: {accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMhRwKFEBLR2tSuRkkSidK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}